import time
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sudachipy import dictionary
from sudachipy import tokenizer
import numpy as np

start_time = time.time()

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šã‹ã‚‰ã€äººé–“å¤±æ ¼ã€åäºŒæ”¯è€ƒã€ã€Œå¥³ã‚‰ã—ã•ã€ã¨ã¯ä½•ã‹ï¼‰
learning = [
    "è‡ªåˆ†ã«ã¯ã€äººé–“ã®ç”Ÿæ´»ã¨ã„ã†ã‚‚ã®ãŒã€è¦‹å½“ã¤ã‹ãªã„ã®ã§ã™ã€‚è‡ªåˆ†ã¯æ±åŒ—ã®ç”°èˆã«ç”Ÿã‚Œã¾ã—ãŸã®ã§ã€æ±½è»Šã‚’ã¯ã˜ã‚ã¦è¦‹ãŸã®ã¯ã€ã‚ˆã»ã©å¤§ãããªã£ã¦ã‹ã‚‰ã§ã—ãŸã€‚",
    "ãƒ‰ã‚¤ãƒ„ã®ãƒãƒ³ãƒãƒ¼ãƒ«ãƒˆå¤¥ã—ãææ–™ã‚’é›†ã‚ã¦ç ”ç©¶ã—ãŸæ‰€ã«æ‹ ã‚Œã°ã€ç©€ç‰©ã®å‘½ã¯ç©€ç‰©ã¨åˆ¥ã«å­˜ã—ã€æ™‚ã¨ã—ã¦æˆ–ã‚‹å‹•ç‰©ã€æ™‚ã¨ã—ã¦ç”·ã€ã‚‚ã—ãã¯å¥³ã€ã¾ãŸå°å…ã®å½¢ã‚’ç¾ã‚ã™ã¨ã„ã†ã®ãŒç©€ç²¾ã®ä¿¡å¿µã ã€‚",
    "ç§ã¯å¥³å­ãŒã€Œå¦Šå¨ ã™ã‚‹ã€ã¨ã„ã†ä¸€äº‹ã‚’é™¤ã‘ã°ã€ç”·å¥³ã®æ€§åˆ¥ã«ç”±ã£ã¦å®¿å‘½çš„ã«èª²ã›ã‚‰ã‚Œã¦ã„ã‚‹åˆ†æ¥­ã¨ã„ã†ã‚‚ã®ã‚’è¦‹å‡ºã™ã“ã¨ãŒå‡ºæ¥ã¾ã›ã‚“ã€‚",
]

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šã‹ã‚‰ã€äººé–“å¤±æ ¼ã€åäºŒæ”¯è€ƒã€ã€Œå¥³ã‚‰ã—ã•ã€ã¨ã¯ä½•ã‹ï¼‰
testing = [
    "è‡ªåˆ†ã®çˆ¶ã¯ã€æ±äº¬ã«ç”¨äº‹ã®å¤šã„ã²ã¨ã§ã—ãŸã®ã§ã€ä¸Šé‡ã®æ¡œæœ¨ç”ºã«åˆ¥è˜ã‚’æŒã£ã¦ã„ã¦ã€æœˆã®å¤§åŠã¯æ±äº¬ã®ãã®åˆ¥è˜ã§æš®ã—ã¦ã„ã¾ã—ãŸã€‚",
    "ã—ã‹ã‚‹ã¨ã“ã‚ã€é»„è‰²ã®è¡£ã‚’ç€ã€é»„ç‰›ã«è»Šã‚’ç‰½ã‹ã›ã¦ä¹—ã‚Šã€å¾“è€…ã“ã¨ã”ã¨ãé»„è‰²ãªäººãŒé€šã‚Šæ›ã‹ã‚Šã€å°å…ã‚’è¦‹ã‚‹ã¨ã™ãªã‚ã¡ç©€è³Šä½•æ•…ã“ã“ã«åã—å±…ã‚‹ã‹ã¨å•ã†ãŸã€‚",
    "ã€Œå¥³ã‚‰ã—ã•ã€ã¨ã„ã†ã‚‚ã®ã¯ã€è¦ã™ã‚‹ã«ç§ã®ã„ã‚ã‚†ã‚‹ã€Œäººé–“æ€§ã€ã«å¸åã—é‚„å…ƒã•ã‚Œã¦ã—ã¾ã†ã‚‚ã®ã§ã™ã€‚",
]

# ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚¯ãƒ©ã‚¹ï¼‰å
class_name = [
    "A ã€äººé–“å¤±æ ¼ã€",
    "B ã€åäºŒæ”¯è€ƒã€",
    "C ã€ã€Œå¥³ã‚‰ã—ã•ã€ã¨ã¯ä½•ã‹ã€",
]
start = time.time()

class Similarity:
    def __init__(self, arr_title):
        self.cv = CountVectorizer(analyzer=self.split_words)
        self.cv.fit(arr_title)

        self.clf = MultinomialNB()
        self.xs = []

        for item in arr_title:
            self.xs.append(self.cv.transform([item]).toarray())
        X = np.concatenate(self.xs)
        self.clf.fit(X, [i for i in range(1, len(learning)+1)])

    def split_words(self, text):
        tokenizer_obj = dictionary.Dictionary().create()
        mode = tokenizer.Tokenizer.SplitMode.C
        return [m.surface() for m in tokenizer_obj.tokenize(text, mode)]

    def get_vocabulary(self):
        return self.cv.vocabulary_

    def get_word_frequency(self, num):
        return self.cv.transform([learning[num]]).toarray()

    def get_words_frequency(self):
        return self.cv.transform(learning).toarray()

    def words_frequency(self, text):
      return self.cv.transform(text).toarray()

    def predict_proba(self, text):
        x = self.cv.transform([text]).toarray()
        return self.clf.predict_proba(x)

    def predict_log_proba(self, text):
        x = self.cv.transform([text]).toarray()
        return self.clf.predict_log_proba(x)


sim = Similarity(learning)
print("ğŸ“šãƒ©ã‚¤ãƒ–ãƒ©ãƒªï¼ˆscikit-learnï¼‰ã§ã®å®Ÿè¡Œçµæœã€()ã¯é¡ä¼¼åº¦ãŒé«˜ã„é †ç•ªã€‚ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¯ã‚«ãƒ†ã‚´ãƒªè¨˜å·ã€ã€ã€ã¯æ›¸åã€‚")

# sklearnã‚’ä½¿ã£ã¦å˜ç´”ãƒ™ã‚¤ã‚ºï¼ˆãƒŠã‚¤ãƒ¼ãƒ–ãƒ™ã‚¤ã‚ºï¼‰ã‚’é©ç”¨
count = 0
for item in testing:
    result = sim.predict_log_proba(item)
    y = np.argsort(result)[::-1]
    print("ğŸ”µã€"+ class_name[count] + "ã€‘ã€Œ" + item + "ã€ã®é¡ä¼¼åº¦ãŒé«˜ã„é †ç•ª")
    count = count + 1
    for ele in range(len(result)):
        e = np.argsort(result)[::-1][ele]
        num = 1
        for i in range(len(e)-1, -1, -1):
            print("(" + str(num) + ")" +
                  class_name[e[i]])
            num = num + 1

elapsed_time = time.time() - start
print("â°å®Ÿè¡Œæ™‚é–“â° " + str(time.time() - start) + "ç§’")
