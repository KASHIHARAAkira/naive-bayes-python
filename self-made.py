import time
from sklearn.naive_bayes import MultinomialNB
from sklearn.feature_extraction.text import CountVectorizer
from sudachipy import dictionary
from sudachipy import tokenizer
import numpy as np

##### _Ï†(ï½¥_ï½¥ è‡ªä½œç®‡æ‰€ #####

start_time = time.time()

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šã‹ã‚‰ã€äººé–“å¤±æ ¼ã€åäºŒæ”¯è€ƒã€ã€Œå¥³ã‚‰ã—ã•ã€ã¨ã¯ä½•ã‹ï¼‰
learning = [
    "è‡ªåˆ†ã«ã¯ã€äººé–“ã®ç”Ÿæ´»ã¨ã„ã†ã‚‚ã®ãŒã€è¦‹å½“ã¤ã‹ãªã„ã®ã§ã™ã€‚è‡ªåˆ†ã¯æ±åŒ—ã®ç”°èˆã«ç”Ÿã‚Œã¾ã—ãŸã®ã§ã€æ±½è»Šã‚’ã¯ã˜ã‚ã¦è¦‹ãŸã®ã¯ã€ã‚ˆã»ã©å¤§ãããªã£ã¦ã‹ã‚‰ã§ã—ãŸã€‚",
    "ãƒ‰ã‚¤ãƒ„ã®ãƒãƒ³ãƒãƒ¼ãƒ«ãƒˆå¤¥ã—ãææ–™ã‚’é›†ã‚ã¦ç ”ç©¶ã—ãŸæ‰€ã«æ‹ ã‚Œã°ã€ç©€ç‰©ã®å‘½ã¯ç©€ç‰©ã¨åˆ¥ã«å­˜ã—ã€æ™‚ã¨ã—ã¦æˆ–ã‚‹å‹•ç‰©ã€æ™‚ã¨ã—ã¦ç”·ã€ã‚‚ã—ãã¯å¥³ã€ã¾ãŸå°å…ã®å½¢ã‚’ç¾ã‚ã™ã¨ã„ã†ã®ãŒç©€ç²¾ã®ä¿¡å¿µã ã€‚",
    "ç§ã¯å¥³å­ãŒã€Œå¦Šå¨ ã™ã‚‹ã€ã¨ã„ã†ä¸€äº‹ã‚’é™¤ã‘ã°ã€ç”·å¥³ã®æ€§åˆ¥ã«ç”±ã£ã¦å®¿å‘½çš„ã«èª²ã›ã‚‰ã‚Œã¦ã„ã‚‹åˆ†æ¥­ã¨ã„ã†ã‚‚ã®ã‚’è¦‹å‡ºã™ã“ã¨ãŒå‡ºæ¥ã¾ã›ã‚“ã€‚",
]

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ï¼ˆä¸Šã‹ã‚‰ã€äººé–“å¤±æ ¼ã€åäºŒæ”¯è€ƒã€ã€Œå¥³ã‚‰ã—ã•ã€ã¨ã¯ä½•ã‹ï¼‰
testing = [
    "è‡ªåˆ†ã®çˆ¶ã¯ã€æ±äº¬ã«ç”¨äº‹ã®å¤šã„ã²ã¨ã§ã—ãŸã®ã§ã€ä¸Šé‡ã®æ¡œæœ¨ç”ºã«åˆ¥è˜ã‚’æŒã£ã¦ã„ã¦ã€æœˆã®å¤§åŠã¯æ±äº¬ã®ãã®åˆ¥è˜ã§æš®ã—ã¦ã„ã¾ã—ãŸã€‚",
    "ã—ã‹ã‚‹ã¨ã“ã‚ã€é»„è‰²ã®è¡£ã‚’ç€ã€é»„ç‰›ã«è»Šã‚’ç‰½ã‹ã›ã¦ä¹—ã‚Šã€å¾“è€…ã“ã¨ã”ã¨ãé»„è‰²ãªäººãŒé€šã‚Šæ›ã‹ã‚Šã€å°å…ã‚’è¦‹ã‚‹ã¨ã™ãªã‚ã¡ç©€è³Šä½•æ•…ã“ã“ã«åã—å±…ã‚‹ã‹ã¨å•ã†ãŸã€‚",
    "ã€Œå¥³ã‚‰ã—ã•ã€ã¨ã„ã†ã‚‚ã®ã¯ã€è¦ã™ã‚‹ã«ç§ã®ã„ã‚ã‚†ã‚‹ã€Œäººé–“æ€§ã€ã«å¸åã—é‚„å…ƒã•ã‚Œã¦ã—ã¾ã†ã‚‚ã®ã§ã™ã€‚",
]

# ã‚«ãƒ†ã‚´ãƒªï¼ˆã‚¯ãƒ©ã‚¹ï¼‰å
class_name = [
    "A ã€äººé–“å¤±æ ¼ã€",
    "B ã€åäºŒæ”¯è€ƒã€",
    "C ã€ã€Œå¥³ã‚‰ã—ã•ã€ã¨ã¯ä½•ã‹ã€",
]

# äº‹å‰ç¢ºç‡ï¼ˆä»Šå›ã¯å…¨ã¦1/3ï¼‰
pC = [1/3, 1/3, 1/3]

# å½¢æ…‹ç´ è§£æã‚’ã™ã‚‹class
class Morphology:
  def __init__(self, arr_title):
    self.data = arr_title
    self.cv = CountVectorizer(analyzer=self.split_words)
    self.cv.fit(arr_title)

    self.clf = MultinomialNB()
    self.xs = []

    for item in arr_title:
        self.xs.append(self.cv.transform([item]).toarray())
    X = np.concatenate(self.xs)
    self.clf.fit(X, [i for i in range(1, len(arr_title)+1)])

  def split_words(self, text):
    tokenizer_obj = dictionary.Dictionary().create()
    mode = tokenizer.Tokenizer.SplitMode.C
    return [m.surface() for m in tokenizer_obj.tokenize(text, mode)]

  def get_vocabulary(self):
    return self.cv.vocabulary_

  def get_words_frequency(self):
    return self.cv.transform(self.data).toarray()

  def get_matched_words(self, text):
    return self.cv.transform(text).toarray()

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã‚’å½¢æ…‹ç´ è§£æã™ã‚‹ãŸã‚ã«åˆæœŸåŒ–
morphology_learning = Morphology(learning)

# å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å˜èªä¸€è¦§
words_list_learning = morphology_learning.get_vocabulary()

# äº‹å‰ç¢ºç‡ï¼ˆå˜èªã®å‡ºç¾é »åº¦ï¼‰ã®å®šç¾©
freq_learning = morphology_learning.get_words_frequency() # å„å˜èªã®å‡ºç¾é »åº¦
total_each_class_learning = np.sum(freq_learning, axis=1) # å„ã‚¯ãƒ©ã‚¹ã®å˜èªæ•°

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®æƒ…å ±ã‚’å–å¾—
morphology_testing = Morphology(testing)

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å˜èªä¸€è¦§
words_list_testing = morphology_testing.get_vocabulary()

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å„å˜èªå‡ºç¾é »åº¦
freq_testing = morphology_testing.get_words_frequency() # å„å˜èªã®å‡ºç¾é »åº¦
total_each_class_testing = np.sum(freq_testing, axis=1)  # å„ã‚¯ãƒ©ã‚¹ã®å˜èªæ•°

# ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã«å‡ºç¾ã™ã‚‹å˜èªã§å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã«å«ã¾ã‚Œã‚‹å˜èªã‚’æŠ½å‡º
matched_words = morphology_learning.get_matched_words(testing)

print("ğŸ“è‡ªä½œå˜ç´”ãƒ™ã‚¤ã‚ºã®çµæœã€()ã¯é¡ä¼¼åº¦ãŒé«˜ã„é †ç•ªã€‚ã‚¢ãƒ«ãƒ•ã‚¡ãƒ™ãƒƒãƒˆã¯ã‚«ãƒ†ã‚´ãƒªè¨˜å·ã€ã€ã€ã¯æ›¸åã€‚")
for j in range(0,len(class_name)):
  result_arr = np.array([])
  for i in range(0,len(class_name)):

    num_arr_matched = np.intersect1d(np.nonzero(freq_learning[i]), np.nonzero(matched_words[j]))

    freq_learning_matched = freq_learning[i][num_arr_matched]  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ä¸€è‡´ã—ãŸå­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã®å‡ºç¾é »åº¦
    num_matched = matched_words[j][num_arr_matched]  # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ä¸€è‡´ã—ãŸãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§ã®å˜èªå‡ºç¾é »åº¦

    len_non_zero_matched_words = np.sum(num_matched)  # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã§å­¦ç¿’ãƒ‡ãƒ¼ã‚¿ã¨ä¸€è‡´ã—ãŸå˜èªã®å‡ºç¾æ•°ï¼ˆåˆ†å­ãŒ0ã§ã¯ãªã„è¦ç´ ã®æ•°ï¼‰
    num_testing_words = total_each_class_testing[j] # ãƒ†ã‚¹ãƒˆãƒ‡ãƒ¼ã‚¿ã®å…¨ä½“ã®å˜èªæ•°, è¡Œåˆ—ã®é•·ã•ã«ãªã‚‹
    len_zero_arr = num_testing_words - len_non_zero_matched_words # è¦ç´ ãŒã‚¼ãƒ­ã®é…åˆ—ã®é•·ã•

    # è·é‡ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°
    # éã‚¼ãƒ­è¦ç´ ã®åˆ†å­å‡¦ç†
    non_zero_smooth = freq_learning_matched + 1

    num = 0
    for item in num_matched:
      non_zero_smooth[num] = non_zero_smooth[num] ** item
      num = num + 1
    # ã‚¼ãƒ­ã®è¦ç´ ã‚’è¶³ã™
    pwc = np.append(non_zero_smooth, np.ones(len_zero_arr)) #åˆ†å­ãŒå‡ºæ¥ãŸ

    # åˆ†æ¯ã‚’ä½œã‚‹
    val_deno = np.sum(freq_learning[i]) # åˆ†æ¯ã¨ãªã‚‹åŸºæœ¬ã®æ•°ï¼ˆè·é‡ã‚¹ãƒ ãƒ¼ã‚¸ãƒ³ã‚°å‰ï¼‰
    len_learning = len(freq_learning[i]) # å­¦ç¿’ãƒ‡ãƒ¼ã‚¿å…¨ä½“ã«å‡ºç¾ã™ã‚‹å˜èªã‹ã‚‰é‡è¤‡ã‚’æŠœã„ãŸæ•°

    deno = np.array([])
    for item in num_matched:
      deno = np.append(deno, (val_deno + len_learning) ** item)
    deno = np.append(deno, np.full(len_zero_arr, (val_deno + len_learning)))
    members = pwc / deno
    result_arr = np.append(result_arr, np.sum(np.log(members)) + np.log(pC[i]))
  print("ğŸŒŸã€" + class_name[j] + "ã€‘ã€Œ" + testing[j] + "ã€ã®é¡ä¼¼åº¦ãŒé«˜ã„é †ç•ª")
  num = 1
  for item in np.argsort(result_arr)[::-1]:
    print("(" + str(num) + ") " + class_name[item])
    num = num + 1
  # print(result_arr) # é¡ä¼¼åº¦ï¼ˆå¯¾æ•°ï¼‰
print("âŒ›å®Ÿè¡Œæ™‚é–“âŒ› " + str(time.time() - start_time) + "ç§’\n")
